{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca028886-0a85-44dd-b7a2-e6131dab42c9",
   "metadata": {},
   "source": [
    "# Design RNN or its variant including LSTM or GRU \n",
    "a) Select a suitable time series dataset. Example – predict sentiments based on product reviews. \n",
    "\n",
    "b) Apply for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525feb09-c797-4b57-b3d0-232ecc98881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553eac3-fa1a-44aa-8bf9-bfa07370cc24",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    " \n",
    "1. Sequential: A linear stack of layers to build the model.\n",
    "\n",
    "2. Embedding: Converts integer-encoded words into dense vectors (e.g., \"cat\" → [0.2, -0.5, ...]).\n",
    "\n",
    "3. LSTM: Layer to process sequential data with memory cells and gates.\n",
    "\n",
    "4. Dense: Fully connected layer for classification.\n",
    "\n",
    "5. imdb: Preloaded dataset of movie reviews labeled as positive (1) or negative (0).\n",
    "\n",
    "6. sequence: Utilities for padding sequences to a fixed length. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc02fa87-8c88-4241-9f8c-81dee52ee504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "vocab_size = 5000  # Use top 5,000 frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35353631-49fa-4b65-b615-eb7222329955",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "vocab_size=5000: Restrict vocabulary to the 5,000 most frequent words (reduces noise from rare words).\n",
    "\n",
    "imdb.load_data(): Loads the IMDB dataset preprocessed into integer sequences.\n",
    "\n",
    "x_train/x_test: Lists of reviews, where each word is replaced by its integer index.\n",
    "\n",
    "y_train/y_test: Labels (0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a59a6449-e250-488e-acc6-b48e26686bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to fixed length (400 words)\n",
    "max_words = 400\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b8907-46bf-45b4-8048-92c4ab34d884",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "max_words=400: Truncate/pad all reviews to 400 words.\n",
    "\n",
    "Shorter reviews are padded with zeros (e.g., [0, 0, ..., 12, 42]).\n",
    "\n",
    "Longer reviews are truncated to 400 words.\n",
    "\n",
    "Why? Neural networks require fixed-length inputs for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0be0fe6-2997-4b68-8a07-85315a7943c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential(name=\"LSTM_Sentiment_Analysis\")\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_words))  # Convert word indices to 32D vectors\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=False))  # 128 LSTM units\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba284fe1-0024-414a-af78-37ee17c1467c",
   "metadata": {},
   "source": [
    " Step 1: Embedding Layer:\n",
    "            vocab_size: 5,000 unique words.\n",
    "            32: Embedding dimension (each word is a 32-dimensional vector).\n",
    "            input_length=max_words: Each input sequence has 400 words.\n",
    "            Purpose: Converts sparse integer-encoded words into dense vectors that capture semantic meaning (e.g., \"good\" and \"great\" are closer in vector space).\n",
    "\n",
    "Step 2: LSTM Layer:\n",
    "            128: Number of LSTM units (dimensionality of the hidden state).\n",
    "            activation='tanh': Hyperbolic tangent activation for gate updates.\n",
    "            return_sequences=False: Return only the final output (not all timesteps).\n",
    "            Purpose: Processes the sequence word-by-word, updating its hidden state to capture context.\n",
    "\n",
    "Step 3: Dense Layer:\n",
    "            1: Single neuron for binary classification (positive/negative).\n",
    "            activation='sigmoid': Squashes output to [0, 1] (probability of positive sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9331ac1b-8965-4fce-8397-9da3915cba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140cc82-6c3b-40de-afd3-1887dcd68ad5",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "loss='binary_crossentropy': Standard loss for binary classification.\n",
    "\n",
    "optimizer='adam': Adaptive learning rate optimizer (efficient for RNNs).\n",
    "\n",
    "metrics=['accuracy']: Track accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1acc9b7-91ca-4d8f-ba34-8daa83af08f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 436ms/step - accuracy: 0.6646 - loss: 0.5730 - val_accuracy: 0.8034 - val_loss: 0.4325\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 487ms/step - accuracy: 0.8721 - loss: 0.3227 - val_accuracy: 0.8582 - val_loss: 0.3358\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 500ms/step - accuracy: 0.9029 - loss: 0.2521 - val_accuracy: 0.8600 - val_loss: 0.3267\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 478ms/step - accuracy: 0.9181 - loss: 0.2118 - val_accuracy: 0.8454 - val_loss: 0.3541\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 457ms/step - accuracy: 0.9273 - loss: 0.1928 - val_accuracy: 0.8640 - val_loss: 0.3388\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(x_train, y_train, \n",
    "                   batch_size=64, \n",
    "                   epochs=5, \n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e0502-604e-4930-a0f7-e67eb7ef0152",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "batch_size=64: Update weights after every 64 samples (balance speed/memory).\n",
    "            \n",
    "epochs=5: Train for 5 full passes over the training data.\n",
    "            \n",
    "validation_split=0.2: Use 20% of training data for validation (monitor overfitting).\n",
    "            \n",
    "Output: Training logs show loss/accuracy for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89ac95cf-fafb-49fb-8496-ea9125260ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.53%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd3a53-774b-443a-9096-594be7a7014a",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "evaluate(): Computes loss and accuracy on unseen test data.\n",
    "\n",
    "test_acc: Accuracy reflects how well the model generalizes to new reviews.\n",
    "            \n",
    "Typical Output: ~80-88% accuracy depending on hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71497d1-f638-4987-ae1e-76abe81df698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
